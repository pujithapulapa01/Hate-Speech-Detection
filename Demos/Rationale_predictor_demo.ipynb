{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rationale_predictor_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBD0BwuKzhbK"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hate-alert/Tutorial-ICWSM-2021/blob/main/Demos/Rationale_predictor_demo.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU55lwj03fsJ"
      },
      "source": [
        "# **Rationale and Label predictor for Abusive speech**\n",
        "> Here, we present a tool which can predict both rationale and labels given a dataset having unknown label. \n",
        "This tool is provided by [Hate-alert](https://github.com/hate-alert)\n",
        "\n",
        "![hate speech](https://www.media-diversity.org/wp-content/uploads/2021/03/shutterstock_1523413514-780x520.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8c3xcqtwe6c"
      },
      "source": [
        "#**Install necessary modules**\n",
        "####this cell will install transformers and other necessary packages required for running the code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKUqwmKu3t_d"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install ekphrasis\n",
        "!git clone https://github.com/hate-alert/Tutorial-ICWSM-2021.git\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkJAnGuN_gLB",
        "outputId": "cb5dbd80-ee5c-4256-ab2e-c938a36d93f3"
      },
      "source": [
        "cd Tutorial-ICWSM-2021"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Tutorial-ICWSM-2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_9MmtLF785n"
      },
      "source": [
        "#### **Import necessary modules**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTB7UXsm3_mF"
      },
      "source": [
        "%%capture\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import BertForTokenClassification, BertForSequenceClassification,BertPreTrainedModel, BertModel\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import re\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from Code.utils import *\n",
        "from Code.model import *\n",
        "from Code.predictions import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd26Eq_pXXP1"
      },
      "source": [
        "### **Set GPU** : \n",
        "> This will select the device based on your current configuration. Select Runtime --> change runtimetype and select GPU as hardware accelerator to use GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjbiE-E6Fl0v"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "   device = torch.device(\"cuda\")\n",
        "else:\n",
        "   device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X47xZEZ3UqwD"
      },
      "source": [
        "# **Model and its origin:-**\n",
        "\n",
        "*  **The model here was trained using the [hatexplain dataset](https://huggingface.co/datasets/hatexplain)** \n",
        "\n",
        "This is a BERT-BASE-UNCASED model trained with label predictor and rationale predictor head\n",
        "\n",
        "> **Labels**  -- abusive (hateful/offensive) and normal labels \n",
        "> **Rationales** -- Highlighted the words which can justify the label selected by the annotators (only for abusive labels)\n",
        "\n",
        "\n",
        "#### **If used cite this**  \n",
        "```\n",
        "@article{mathew2020hatexplain,\n",
        "  title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},\n",
        "  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},\n",
        "  journal={arXiv preprint arXiv:2012.10289},\n",
        "  year={2020}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8_FUrHCC5FQ"
      },
      "source": [
        "model = modelPredRationale(model_path='Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two',device=device)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M-ma7nPBHL7"
      },
      "source": [
        "#@title **How do you want to enter text ?**\n",
        "# @markdown You can either directly enter the text (text input) or uppload from a csv (file)\n",
        "input_type = \"text input\" #@param [\"file\", \"text input\"]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Ut_xu0BV3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d2cf03-3c64-4eb2-fbb1-a4fce38f141c"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "if input_type == \"text input\":\n",
        "    text_input = input(\"Write the post: \")\n",
        "    dataset=[text_input]\n",
        "else:\n",
        "  print(\"Please upload the csv file you want to get predictions\")\n",
        "  print(\"Please make sure the column name of the csv should be Index, Sentences\")\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "  dataset = pd.read_csv(io.BytesIO(uploaded[list(uploaded)[0]])).reset_index()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Write the post: you deserve death if you voted yes cruz that dumbass spic was destined to lose\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en7viZPEmF3j"
      },
      "source": [
        "def getDatasetPrediction(dataset,config):\n",
        "    labels,attention,sents=model.return_rationales(dataset['Sentences'])\n",
        "    predictions = {}\n",
        "    for index, row in dataset.iterrows():\n",
        "        dict1={}\n",
        "        dict1['Sentence']=row['Sentences']\n",
        "        dict_labels={}\n",
        "        for ele in config:\n",
        "            dict_labels[config[ele]]=round(labels[index][ele],3)\n",
        "        dict1[\"Labels\"]=dict_labels\n",
        "\n",
        "        dict1[\"Tokens\"] = sents[index]\n",
        "        if (np.argmax(labels[index])==0):\n",
        "          dict1[\"Rationale\"]=list(np.zeros(len(sents[index])))\n",
        "        else:\n",
        "          dict1[\"Rationale\"]=attention[index][0:len(dict1[\"Tokens\"])]\n",
        "        predictions[row['Index']] = dict1\n",
        "    return predictions\n",
        "\n",
        "def getRandomTextFromPred(pred = None):\n",
        "    return random.choice(list(pred.items()))"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_93XaDK9oIU6",
        "outputId": "f3f1c657-0c37-469b-8b2a-67961fbbe788"
      },
      "source": [
        "if input_type == \"text input\":\n",
        "    labels,attention,sents=model.return_rationales(dataset)\n",
        "    \n",
        "else:\n",
        "    config=model.config.id2label\n",
        "    predictions= getDatasetPrediction(dataset,config)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running eval on test data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAUm4mfCZPrG"
      },
      "source": [
        "def show_attention_rationale(tokenized_text,attention_vector):\n",
        "    char_vals = [CharVal(c, v) for c, v in zip(tokenized_text, attention_vector)]\n",
        "    char_df = pd.DataFrame(char_vals).transpose()\n",
        "    char_df = char_df.style.applymap(color_charvals_rationale)\n",
        "    return char_df"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo4gQwHN_Ewc"
      },
      "source": [
        "### Show a random text with rationales, No rationales will mean the text is not predicted as abusive by the model.\n",
        "if input_type != \"text input\":\n",
        "    pred=getRandomTextFromPred(predictions)\n",
        "    print(pred)\n",
        "    char_df=show_attention_rationale(pred[1]['Tokens'],pred[1]['Rationale'])\n",
        "else:\n",
        "    pred= {'Normal':labels[0][0], 'Abusive':labels[0][1]}\n",
        "    if (np.argmax(labels[0])==0):\n",
        "        attention=[list(np.zeros(len(sents[0])))]\n",
        "    else:\n",
        "        pass    \n",
        "    char_df=show_attention_rationale(sents[0], attention[0])"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "id": "sp0aMAhszN-G",
        "outputId": "dc8b4100-1606-492c-f502-fad26d81a1a2"
      },
      "source": [
        "print(\"Prediction probablities:\", pred)\n",
        "char_df"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction probablities: {'Normal': 0.026733398, 'Abusive': 0.9732666}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col0,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col5,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col6,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col7,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col8,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col9,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col17,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col18{\n",
              "            background-color:  #ffffff;\n",
              "        }#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col1,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col4,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col15{\n",
              "            background-color:  #fffdfd;\n",
              "        }#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col2,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col10{\n",
              "            background-color:  #fff7f7;\n",
              "        }#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col3,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col11{\n",
              "            background-color:  #fff3f3;\n",
              "        }#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col12{\n",
              "            background-color:  #ff9f9f;\n",
              "        }#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col13{\n",
              "            background-color:  #ff9a9a;\n",
              "        }#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col14,#T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col16{\n",
              "            background-color:  #fffefe;\n",
              "        }</style><table id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>        <th class=\"col_heading level0 col9\" >9</th>        <th class=\"col_heading level0 col10\" >10</th>        <th class=\"col_heading level0 col11\" >11</th>        <th class=\"col_heading level0 col12\" >12</th>        <th class=\"col_heading level0 col13\" >13</th>        <th class=\"col_heading level0 col14\" >14</th>        <th class=\"col_heading level0 col15\" >15</th>        <th class=\"col_heading level0 col16\" >16</th>        <th class=\"col_heading level0 col17\" >17</th>        <th class=\"col_heading level0 col18\" >18</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col0\" class=\"data row0 col0\" >[CLS]</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col1\" class=\"data row0 col1\" >you</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col2\" class=\"data row0 col2\" >deserve</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col3\" class=\"data row0 col3\" >death</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col4\" class=\"data row0 col4\" >if</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col5\" class=\"data row0 col5\" >you</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col6\" class=\"data row0 col6\" >voted</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col7\" class=\"data row0 col7\" >yes</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col8\" class=\"data row0 col8\" >cruz</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col9\" class=\"data row0 col9\" >that</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col10\" class=\"data row0 col10\" >dumb</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col11\" class=\"data row0 col11\" >##ass</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col12\" class=\"data row0 col12\" >sp</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col13\" class=\"data row0 col13\" >##ic</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col14\" class=\"data row0 col14\" >was</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col15\" class=\"data row0 col15\" >destined</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col16\" class=\"data row0 col16\" >to</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col17\" class=\"data row0 col17\" >lose</td>\n",
              "                        <td id=\"T_af3fb71a_c69f_11eb_a93f_0242ac1c0002row0_col18\" class=\"data row0 col18\" >[SEP]</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f1b530715d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "debOLbbeV8aj"
      },
      "source": [
        "### **Word of caution**\n",
        "\n",
        "> Model used here have any trained using a particular dataset and they may carry some bias or errors, they should be only used as a complementary labels in case of any analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn2NNW1e1wWT"
      },
      "source": [
        "#**Download the file generated**\n",
        "#### Run this cell and select the destination folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki5ACvSoCe2l",
        "outputId": "45125ca0-f41a-452b-d166-2467b6056067"
      },
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "if input_type != \"text input\":\n",
        "  with open('predictions.json', 'w') as f:\n",
        "      json_string = json.dumps(predictions, cls=NumpyEncoder, sort_keys=True, indent=4)\n",
        "      f.write(json_string)\n",
        "  files.download('predictions.json')\n",
        "else:\n",
        "  print(\"No file input given\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No file input given\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNZr_e0IErnl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}